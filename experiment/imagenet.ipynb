{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imagenet Experiment\n",
    "\n",
    "## Goal \n",
    "This script will first extract top 1k keywords which is most frequent and available at imagenet.  Then this script will download images for each keywords and then generate captions for each image.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "import hickle\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.corpus import wordnet as wn\n",
    "import urllib\n",
    "import tarfile\n",
    "from PIL import Image\n",
    "from core.vggnet import Vgg19\n",
    "import tensorflow as tf\n",
    "from scipy import ndimage\n",
    "from core.solver import CaptioningSolver\n",
    "from core.model import CaptionGenerator\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "caption_file = 'data/annotations/captions_train2014.json'\n",
    "image_dir = 'image/train2014_resized'\n",
    "max_length = 15\n",
    "word_count_threshold = 100\n",
    "vgg_model_path = './data/imagenet-vgg-verydeep-19.mat'\n",
    "batch_size = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary Building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _process_caption_data(caption_file, image_dir, max_length):\n",
    "    with open(caption_file) as f:\n",
    "        caption_data = json.load(f)\n",
    "\n",
    "    # id_to_filename is a dictionary such as {image_id: filename]} \n",
    "    id_to_filename = {image['id']: image['file_name'] for image in caption_data['images']}\n",
    "\n",
    "    # data is a list of dictionary which contains 'captions', 'file_name' and 'image_id' as key.\n",
    "    data = []\n",
    "    for annotation in caption_data['annotations']:\n",
    "        image_id = annotation['image_id']\n",
    "        annotation['file_name'] = os.path.join(image_dir, id_to_filename[image_id])\n",
    "        data += [annotation]\n",
    "\n",
    "    # convert to pandas dataframe (for later visualization or debugging)\n",
    "    caption_data = pd.DataFrame.from_dict(data)\n",
    "    del caption_data['id']\n",
    "    caption_data.sort_values(by='image_id', inplace=True)\n",
    "    caption_data = caption_data.reset_index(drop=True)\n",
    "\n",
    "    del_idx = []\n",
    "    for i, caption in enumerate(caption_data['caption']):\n",
    "        caption = caption.replace('.', '').replace(',', '').replace(\"'\", \"\").replace('\"', '')\n",
    "        caption = caption.replace('&', 'and').replace('(', '').replace(\")\", \"\").replace('-', ' ')\n",
    "        caption = \" \".join(caption.split())  # replace multiple spaces\n",
    "\n",
    "        caption_data.set_value(i, 'caption', caption.lower())\n",
    "        if len(caption.split(\" \")) > max_length:\n",
    "            del_idx.append(i)\n",
    "\n",
    "    # delete captions if size is larger than max_length\n",
    "    print \"The number of captions before deletion: %d\" % len(caption_data)\n",
    "    caption_data = caption_data.drop(caption_data.index[del_idx])\n",
    "    caption_data = caption_data.reset_index(drop=True)\n",
    "    print \"The number of captions after deletion: %d\" % len(caption_data)\n",
    "    return caption_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _build_vocab(annotations, threshold=1):\n",
    "    counter = Counter()\n",
    "    max_len = 0\n",
    "    for i, caption in enumerate(annotations['caption']):\n",
    "        words = caption.split(' ') # caption contrains only lower-case words\n",
    "        for w in words:\n",
    "            counter[w] +=1\n",
    "        \n",
    "        if len(caption.split(\" \")) > max_len:\n",
    "            max_len = len(caption.split(\" \"))\n",
    "\n",
    "    vocab = [word for word in counter if counter[word] >= threshold]\n",
    "    print ('Filtered %d words to %d words with word count threshold %d.' % (len(counter), len(vocab), threshold))\n",
    "\n",
    "    word_to_idx = {u'<NULL>': 0, u'<START>': 1, u'<END>': 2}\n",
    "    idx = 3\n",
    "    for word in vocab:\n",
    "        word_to_idx[word] = idx\n",
    "        idx += 1\n",
    "    print \"Max length of caption: \", max_len\n",
    "    return word_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _build_top1k_vocab(annotations):\n",
    "    counter = Counter()\n",
    "    for i, caption in enumerate(annotations['caption']):\n",
    "        words = caption.split(' ') # caption contrains only lower-case words\n",
    "        for w in words:\n",
    "            counter[w] +=1\n",
    "    #Read imagenet synsets\n",
    "    with open('./data/imagenet.synsets','r') as f:\n",
    "        synsets = f.readlines()\n",
    "    imagenet_synsets = { w.rstrip():True for w in synsets}\n",
    "    \n",
    "    top1k = []\n",
    "    frequentWords = counter.most_common()\n",
    "    i = 0\n",
    "    \n",
    "    while len(top1k) < 1000 and i< len(frequentWords):\n",
    "        w = frequentWords[i][0]\n",
    "        ss = wn.synsets(frequentWords[i][0])\n",
    "        j = 0\n",
    "        while j< len(ss) and ss[j].pos() != 'n': j += 1\n",
    "        if j<len(ss):\n",
    "            wnid = ss[j].pos() + str(ss[j].offset()).zfill(8)\n",
    "            if wnid in imagenet_synsets:\n",
    "                top1k.append((wnid, w))\n",
    "        i += 1\n",
    "    return top1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "a = _build_top1k_vocab(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wn.synsets('his')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset = _process_caption_data(caption_file=caption_file,\n",
    "                                      image_dir=image_dir,\n",
    "                                      max_length=max_length)\n",
    "                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_to_idx = _build_vocab(annotations=train_dataset, threshold=word_count_threshold)\n",
    "save_pickle(word_to_idx, './data/word_to_idx.pkl')\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top1k = _build_top1k_vocab(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "top1k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ss = wn.synsets('hats')\n",
    "ss[0].offset()\n",
    "wnid = ss[0].pos() + str(ss[0].offset()).zfill(8)\n",
    "print wnid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ss= wn.synsets('hats')\n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pre_url = 'http://www.image-net.org/download/synset?wnid='\n",
    "post_url = '&username=intuinno&accesskey=6be8155ee3d56b5120241b3bda13412d3cc0cd42&release=latest&src=stanford'\n",
    "testfile = urllib.URLopener()\n",
    "testfile.retrieve(pre_url+wnid+post_url, wnid+'.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cur_dir = os.getcwd()\n",
    "original_dir = './data/imagenet/%s/original/'%wnid\n",
    "resized_dir = './data/imagenet/%s/resized/'%wnid\n",
    "\n",
    "if not os.path.exists(wnid):\n",
    "    os.makedirs(original_dir)\n",
    "    os.rename(wnid+'.tar', original_dir + 'data.tar' )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(original_dir)\n",
    "print os.getcwd()\n",
    "tar = tarfile.open('data.tar')\n",
    "tar.extractall()\n",
    "tar.close()\n",
    "os.remove('data.tar')\n",
    "os.chdir(cur_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def resize_image(image):\n",
    "    width, height = image.size\n",
    "    if width > height:\n",
    "        left = (width - height) / 2\n",
    "        right = width - left\n",
    "        top = 0\n",
    "        bottom = height\n",
    "    else:\n",
    "        top = (height - width) /2\n",
    "        bottom = height - top\n",
    "        left = 0\n",
    "        right = width \n",
    "    image = image.crop((left, top, right, bottom))\n",
    "    image = image.resize([224,224], Image.ANTIALIAS)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(resized_dir):\n",
    "    os.makedirs(resized_dir)\n",
    "print 'Start resizing %s images.' %wnid\n",
    "image_files = os.listdir(original_dir)\n",
    "for i, image_file in enumerate(image_files):\n",
    "#     from IPython.core.debugger import Tracer; Tracer()() \n",
    "    with open(os.path.join(original_dir, image_file),'r+b') as f:\n",
    "        image = Image.open(f)\n",
    "        image = resize_image(image)\n",
    "        image.save(os.path.join(resized_dir, image_file), image.format)\n",
    "        if i % 100 == 0:\n",
    "            print 'Resized images: %d/%d' %(i, len(image_files))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vggnet = Vgg19(vgg_model_path)\n",
    "vggnet.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    n_examples = len(image_files)\n",
    "    all_feats = np.ndarray([n_examples, 196,512], dtype=np.float32)\n",
    "    \n",
    "    for start, end in zip(range(0, n_examples, batch_size),\n",
    "                          range(batch_size, n_examples+batch_size, batch_size)):\n",
    "        image_batch_file = image_files[start:end]\n",
    "        image_batch = np.array(map(lambda x: ndimage.imread(os.path.join(resized_dir, x), mode='RGB'), image_batch_file)).astype(np.float32)\n",
    "        feats = sess.run(vggnet.features, feed_dict={vggnet.images: image_batch})\n",
    "        all_feats[start:end, :] = feats\n",
    "        print (\"Processed %d %s features\" %(end, wnid))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_path = './data/%s.hkl' %wnid\n",
    "hickle.dump(all_feats, save_path)\n",
    "print \"Saved %s..\" % save_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Run model to generate Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "import tensorflow as tf\n",
    "from core.solver import CaptioningSolver\n",
    "from core.model import CaptionGenerator\n",
    "from nltk.corpus import wordnet as wn\n",
    "import hickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ss = wn.synsets('hats')\n",
    "ss[0].offset()\n",
    "wnid = ss[0].pos() + str(ss[0].offset()).zfill(8)\n",
    "print wnid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./data/word_to_idx.pkl','rb') as f:\n",
    "    word_to_idx = pickle.load(f)\n",
    "    \n",
    "with open('./data/%s.hkl'%wnid, 'r') as f:\n",
    "    data = {}\n",
    "    data['features'] = hickle.load(f)\n",
    "    features = data['features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = CaptionGenerator(word_to_idx, dim_feature=[196, 512], dim_embed=512,\n",
    "                                   dim_hidden=1024, n_time_step=16, prev2out=True, \n",
    "                                             ctx2out=True, alpha_c=1.0, selector=True, dropout=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "solver = CaptioningSolver(model, data, data, n_epochs=15, batch_size=128, update_rule='adam',\n",
    "                                      learning_rate=0.0025, print_every=2000, save_every=1, image_path='./data/imagenet/n03487657',\n",
    "                                pretrained_model=None, model_path='./data/model/attention', test_model='./data/model/attention/model-18',\n",
    "                                 print_bleu=False, log_path='./log/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from IPython.core.debugger import Tracer\n",
    "Tracer()() #this one triggers the debugger\n",
    "captions = solver.test_imagenet(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(captions)\n",
    "captions[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.get_variable_scope().reuse_variables()\n",
    "caption2  = solver.test_imagenet(features)\n",
    "caption2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ipdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Top 1k Noun Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _build_top1k_vocab(annotations):\n",
    "    counter = Counter()\n",
    "    for i, caption in enumerate(annotations['caption']):\n",
    "        words = caption.split(' ') # caption contrains only lower-case words\n",
    "        for w in words:\n",
    "            counter[w] +=1\n",
    "\n",
    "    return sorted(Counter[w], key=lambda (k, v): v)[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = _process_caption_data(caption_file=caption_file,\n",
    "                                      image_dir=image_dir,\n",
    "                                      max_length=max_length)\n",
    "top1k = _build_top1k_vocab(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
